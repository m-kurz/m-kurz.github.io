@inproceedings{blind_performance_2024,
 abstract = {Large-scale simulations pose significant challenges not only to the solver itself but also to the pre- and postprocessing framework. Hence, we present generally applicable improvements to enhance the performance of those tools and thus increase the feasibility of large-scale jobs and convergence studies. To accomplish this, we use a shared memory approach implemented in the Message Passing Interface (MPI) libraries. Additionally, we improve the read and write performance of the flow solver during runtime to minimize the load imposed on the file system. A detailed discussion of the current performance and scaling behavior is given for up to 262144 processes. FLEXI shows excellent scalability for all tested features. We conclude by showing selected applications, where we use the introduced improvements to maximize performance.},
 address = {Cham},
 author = {Blind, Marcel and Kopper, Patrick and Kempf, Daniel and Kurz, Marius and Schwarz, Anna and Munz, Claus-Dieter and Beck, Andrea},
 booktitle = {High Performance Computing in Science and Engineering '22},
 doi = {10.1007/978-3-031-46870-4_17},
 editor = {Nagel, Wolfgang E. and Kr√∂ner, Dietmar H. and Resch, Michael M.},
 isbn = {978-3-031-46870-4},
 language = {en},
 pages = {249--264},
 publisher = {Springer Nature Switzerland},
 title = {Performance Improvements for Large-Scale Simulations using the Discontinuous Galerkin Framework FLEXI},
 year = {2024}
}
